{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## NLTK 停止词"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "{'a',\n 'about',\n 'above',\n 'after',\n 'again',\n 'against',\n 'ain',\n 'all',\n 'am',\n 'an',\n 'and',\n 'any',\n 'are',\n 'aren',\n \"aren't\",\n 'as',\n 'at',\n 'be',\n 'because',\n 'been',\n 'before',\n 'being',\n 'below',\n 'between',\n 'both',\n 'but',\n 'by',\n 'can',\n 'couldn',\n \"couldn't\",\n 'd',\n 'did',\n 'didn',\n \"didn't\",\n 'do',\n 'does',\n 'doesn',\n \"doesn't\",\n 'doing',\n 'don',\n \"don't\",\n 'down',\n 'during',\n 'each',\n 'few',\n 'for',\n 'from',\n 'further',\n 'had',\n 'hadn',\n \"hadn't\",\n 'has',\n 'hasn',\n \"hasn't\",\n 'have',\n 'haven',\n \"haven't\",\n 'having',\n 'he',\n 'her',\n 'here',\n 'hers',\n 'herself',\n 'him',\n 'himself',\n 'his',\n 'how',\n 'i',\n 'if',\n 'in',\n 'into',\n 'is',\n 'isn',\n \"isn't\",\n 'it',\n \"it's\",\n 'its',\n 'itself',\n 'just',\n 'll',\n 'm',\n 'ma',\n 'me',\n 'mightn',\n \"mightn't\",\n 'more',\n 'most',\n 'mustn',\n \"mustn't\",\n 'my',\n 'myself',\n 'needn',\n \"needn't\",\n 'no',\n 'nor',\n 'not',\n 'now',\n 'o',\n 'of',\n 'off',\n 'on',\n 'once',\n 'only',\n 'or',\n 'other',\n 'our',\n 'ours',\n 'ourselves',\n 'out',\n 'over',\n 'own',\n 're',\n 's',\n 'same',\n 'shan',\n \"shan't\",\n 'she',\n \"she's\",\n 'should',\n \"should've\",\n 'shouldn',\n \"shouldn't\",\n 'so',\n 'some',\n 'such',\n 't',\n 'than',\n 'that',\n \"that'll\",\n 'the',\n 'their',\n 'theirs',\n 'them',\n 'themselves',\n 'then',\n 'there',\n 'these',\n 'they',\n 'this',\n 'those',\n 'through',\n 'to',\n 'too',\n 'under',\n 'until',\n 'up',\n 've',\n 'very',\n 'was',\n 'wasn',\n \"wasn't\",\n 'we',\n 'were',\n 'weren',\n \"weren't\",\n 'what',\n 'when',\n 'where',\n 'which',\n 'while',\n 'who',\n 'whom',\n 'why',\n 'will',\n 'with',\n 'won',\n \"won't\",\n 'wouldn',\n \"wouldn't\",\n 'y',\n 'you',\n \"you'd\",\n \"you'll\",\n \"you're\",\n \"you've\",\n 'your',\n 'yours',\n 'yourself',\n 'yourselves'}"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(stopwords.words(\"english\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## NLTK 词干提取"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\n",
      "python\n",
      "python\n",
      "python\n",
      "pythonli\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "ps = PorterStemmer() # porter\n",
    "\n",
    "example_words = [\"python\",\"pythoner\",\"pythoning\",\"pythoned\",\"pythonly\"]\n",
    "for w in example_words:\n",
    "    print(ps.stem(w))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## NLTK 词性标注\n",
    "PunktSentenceTokenizer 无监督进行机器学习 进行句子标记"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"PRESIDENT GEORGE W. BUSH'S ADDRESS BEFORE A JOINT SESSION OF THE CONGRESS ON THE STATE OF THE UNION\\n \\nJanuary 31, 2006\\n\\nTHE PRESIDENT: Thank you all.\", 'Mr. Speaker, Vice President Cheney, members of Congress, members of the Supreme Court and diplomatic corps, distinguished guests, and fellow citizens: Today our nation lost a beloved, graceful, courageous woman who called America to its founding ideals and carried on a noble dream.', 'Tonight we are comforted by the hope of a glad reunion with the husband who was taken so long ago, and we are grateful for the good life of Coretta Scott King.', '(Applause.)', 'President George W. Bush reacts to applause during his State of the Union Address at the Capitol, Tuesday, Jan.']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized[:5])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('PRESIDENT', 'NNP'), ('GEORGE', 'NNP'), ('W.', 'NNP'), ('BUSH', 'NNP'), (\"'S\", 'POS'), ('ADDRESS', 'NNP'), ('BEFORE', 'IN'), ('A', 'NNP'), ('JOINT', 'NNP'), ('SESSION', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('CONGRESS', 'NNP'), ('ON', 'NNP'), ('THE', 'NNP'), ('STATE', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('UNION', 'NNP'), ('January', 'NNP'), ('31', 'CD'), (',', ','), ('2006', 'CD'), ('THE', 'NNP'), ('PRESIDENT', 'NNP'), (':', ':'), ('Thank', 'NNP'), ('you', 'PRP'), ('all', 'DT'), ('.', '.')]\n",
      "[('Mr.', 'NNP'), ('Speaker', 'NNP'), (',', ','), ('Vice', 'NNP'), ('President', 'NNP'), ('Cheney', 'NNP'), (',', ','), ('members', 'NNS'), ('of', 'IN'), ('Congress', 'NNP'), (',', ','), ('members', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('Supreme', 'NNP'), ('Court', 'NNP'), ('and', 'CC'), ('diplomatic', 'JJ'), ('corps', 'NN'), (',', ','), ('distinguished', 'JJ'), ('guests', 'NNS'), (',', ','), ('and', 'CC'), ('fellow', 'JJ'), ('citizens', 'NNS'), (':', ':'), ('Today', 'VB'), ('our', 'PRP$'), ('nation', 'NN'), ('lost', 'VBD'), ('a', 'DT'), ('beloved', 'VBN'), (',', ','), ('graceful', 'JJ'), (',', ','), ('courageous', 'JJ'), ('woman', 'NN'), ('who', 'WP'), ('called', 'VBD'), ('America', 'NNP'), ('to', 'TO'), ('its', 'PRP$'), ('founding', 'NN'), ('ideals', 'NNS'), ('and', 'CC'), ('carried', 'VBD'), ('on', 'IN'), ('a', 'DT'), ('noble', 'JJ'), ('dream', 'NN'), ('.', '.')]\n",
      "[('Tonight', 'NN'), ('we', 'PRP'), ('are', 'VBP'), ('comforted', 'VBN'), ('by', 'IN'), ('the', 'DT'), ('hope', 'NN'), ('of', 'IN'), ('a', 'DT'), ('glad', 'JJ'), ('reunion', 'NN'), ('with', 'IN'), ('the', 'DT'), ('husband', 'NN'), ('who', 'WP'), ('was', 'VBD'), ('taken', 'VBN'), ('so', 'RB'), ('long', 'RB'), ('ago', 'RB'), (',', ','), ('and', 'CC'), ('we', 'PRP'), ('are', 'VBP'), ('grateful', 'JJ'), ('for', 'IN'), ('the', 'DT'), ('good', 'JJ'), ('life', 'NN'), ('of', 'IN'), ('Coretta', 'NNP'), ('Scott', 'NNP'), ('King', 'NNP'), ('.', '.')]\n",
      "[('(', '('), ('Applause', 'NNP'), ('.', '.'), (')', ')')]\n",
      "[('President', 'NNP'), ('George', 'NNP'), ('W.', 'NNP'), ('Bush', 'NNP'), ('reacts', 'VBZ'), ('to', 'TO'), ('applause', 'VB'), ('during', 'IN'), ('his', 'PRP$'), ('State', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('Union', 'NNP'), ('Address', 'NNP'), ('at', 'IN'), ('the', 'DT'), ('Capitol', 'NNP'), (',', ','), ('Tuesday', 'NNP'), (',', ','), ('Jan', 'NNP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[:5]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            print(tagged)\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "process_content()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## NLTK 分块\n",
    "分块是要将词汇分成有意义的块，将一些短语分在一组（如名词短语）"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (Chunk PRESIDENT/NNP GEORGE/NNP W./NNP BUSH/NNP)\n",
      "  'S/POS\n",
      "  (Chunk ADDRESS/NNP)\n",
      "  BEFORE/IN\n",
      "  (Chunk A/NNP JOINT/NNP SESSION/NNP)\n",
      "  OF/IN\n",
      "  (Chunk THE/NNP CONGRESS/NNP ON/NNP THE/NNP STATE/NNP)\n",
      "  OF/IN\n",
      "  (Chunk THE/NNP UNION/NNP January/NNP)\n",
      "  31/CD\n",
      "  ,/,\n",
      "  2006/CD\n",
      "  (Chunk THE/NNP PRESIDENT/NNP)\n",
      "  :/:\n",
      "  (Chunk Thank/NNP)\n",
      "  you/PRP\n",
      "  all/DT\n",
      "  ./.)\n",
      "(Chunk PRESIDENT/NNP GEORGE/NNP W./NNP BUSH/NNP)\n",
      "(Chunk ADDRESS/NNP)\n",
      "(Chunk A/NNP JOINT/NNP SESSION/NNP)\n",
      "(Chunk THE/NNP CONGRESS/NNP ON/NNP THE/NNP STATE/NNP)\n",
      "(Chunk THE/NNP UNION/NNP January/NNP)\n",
      "(Chunk THE/NNP PRESIDENT/NNP)\n",
      "(Chunk Thank/NNP)\n",
      "(S\n",
      "  (Chunk Mr./NNP Speaker/NNP)\n",
      "  ,/,\n",
      "  (Chunk Vice/NNP President/NNP Cheney/NNP)\n",
      "  ,/,\n",
      "  members/NNS\n",
      "  of/IN\n",
      "  (Chunk Congress/NNP)\n",
      "  ,/,\n",
      "  members/NNS\n",
      "  of/IN\n",
      "  the/DT\n",
      "  (Chunk Supreme/NNP Court/NNP)\n",
      "  and/CC\n",
      "  diplomatic/JJ\n",
      "  corps/NN\n",
      "  ,/,\n",
      "  distinguished/JJ\n",
      "  guests/NNS\n",
      "  ,/,\n",
      "  and/CC\n",
      "  fellow/JJ\n",
      "  citizens/NNS\n",
      "  :/:\n",
      "  Today/VB\n",
      "  our/PRP$\n",
      "  nation/NN\n",
      "  lost/VBD\n",
      "  a/DT\n",
      "  beloved/VBN\n",
      "  ,/,\n",
      "  graceful/JJ\n",
      "  ,/,\n",
      "  courageous/JJ\n",
      "  woman/NN\n",
      "  who/WP\n",
      "  (Chunk called/VBD America/NNP)\n",
      "  to/TO\n",
      "  its/PRP$\n",
      "  founding/NN\n",
      "  ideals/NNS\n",
      "  and/CC\n",
      "  carried/VBD\n",
      "  on/IN\n",
      "  a/DT\n",
      "  noble/JJ\n",
      "  dream/NN\n",
      "  ./.)\n",
      "(Chunk Mr./NNP Speaker/NNP)\n",
      "(Chunk Vice/NNP President/NNP Cheney/NNP)\n",
      "(Chunk Congress/NNP)\n",
      "(Chunk Supreme/NNP Court/NNP)\n",
      "(Chunk called/VBD America/NNP)\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
    "\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[:2]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            print(chunked)\n",
    "            for subtree in chunked.subtrees(filter=lambda t: t.label() == 'Chunk'):\n",
    "                print(subtree)\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "process_content()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## NLTK添加缝隙 Chinking\n",
    "所谓的添加缝隙就是说从分块中删除不需要的词或块"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[5:8]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "\n",
    "            chunkGram = r\"\"\"Chunk: {<.*>+}\n",
    "                                    }<VB.?|IN|DT|TO>+{\"\"\"\n",
    "\n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "\n",
    "            chunked.draw()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "process_content()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## NLTK命名实体识别\n",
    "识别文本中的所有命名实体或将命名实体识别为命名实体的类型"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S 31/CD ,/, 2006/CD ./.)\n",
      "(S\n",
      "  (NE White/NNP House/NNP)\n",
      "  photo/NN\n",
      "  by/IN\n",
      "  (NE Eric/NNP)\n",
      "  DraperEvery/NNP\n",
      "  time/NN\n",
      "  I/PRP\n",
      "  'm/VBP\n",
      "  invited/JJ\n",
      "  to/TO\n",
      "  this/DT\n",
      "  rostrum/NN\n",
      "  ,/,\n",
      "  I/PRP\n",
      "  'm/VBP\n",
      "  humbled/VBN\n",
      "  by/IN\n",
      "  the/DT\n",
      "  privilege/NN\n",
      "  ,/,\n",
      "  and/CC\n",
      "  mindful/NN\n",
      "  of/IN\n",
      "  the/DT\n",
      "  history/NN\n",
      "  we/PRP\n",
      "  've/VBP\n",
      "  seen/VBN\n",
      "  together/RB\n",
      "  ./.)\n",
      "(S\n",
      "  We/PRP\n",
      "  have/VBP\n",
      "  gathered/VBN\n",
      "  under/IN\n",
      "  this/DT\n",
      "  Capitol/NNP\n",
      "  dome/NN\n",
      "  in/IN\n",
      "  moments/NNS\n",
      "  of/IN\n",
      "  national/JJ\n",
      "  mourning/NN\n",
      "  and/CC\n",
      "  national/JJ\n",
      "  achievement/NN\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[5:8]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            namedEnt = nltk.ne_chunk(tagged, binary=True)\n",
    "            print(namedEnt)\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "\n",
    "process_content()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## NLTK 词形还原\n",
    "词形还原和词干提取类似\n",
    "词干提取可能会创造出不一样的词汇\n",
    "词形还原一定是实际的词汇"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n",
      "cactus\n",
      "goose\n",
      "rock\n",
      "python\n",
      "good\n",
      "best\n",
      "run\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(lemmatizer.lemmatize(\"cats\"))\n",
    "print(lemmatizer.lemmatize(\"cacti\"))\n",
    "print(lemmatizer.lemmatize(\"geese\"))\n",
    "print(lemmatizer.lemmatize(\"rocks\"))\n",
    "print(lemmatizer.lemmatize(\"python\"))\n",
    "print(lemmatizer.lemmatize(\"better\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"best\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"run\"))\n",
    "print(lemmatizer.lemmatize(\"run\",'v'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plan.n.01\n",
      "plan\n",
      "a series of steps to be carried out or goals to be accomplished\n",
      "['they drew up a six-step plan', 'they discussed plans for a new bond issue']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "syns = wordnet.synsets(\"program\")\n",
    "print(syns[0].name())\n",
    "print(syns[0].lemmas()[0].name())\n",
    "print(syns[0].definition())\n",
    "print(syns[0].examples())\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'goodness', 'near', 'expert', 'right', 'safe', 'good', 'just', 'sound', 'undecomposed', 'ripe', 'upright', 'dependable', 'in_force', 'soundly', 'secure', 'thoroughly', 'proficient', 'full', 'skillful', 'well', 'dear', 'trade_good', 'unspoiled', 'estimable', 'serious', 'honorable', 'skilful', 'commodity', 'effective', 'practiced', 'unspoilt', 'salutary', 'adept', 'honest', 'beneficial', 'in_effect', 'respectable'}\n",
      "{'ill', 'evilness', 'evil', 'badness', 'bad'}\n"
     ]
    }
   ],
   "source": [
    "synonyms = []\n",
    "antonyms = []\n",
    "\n",
    "for syn in wordnet.synsets(\"good\"):\n",
    "    for l in syn.lemmas():\n",
    "        synonyms.append(l.name())\n",
    "        if l.antonyms():\n",
    "            antonyms.append(l.antonyms()[0].name())\n",
    "\n",
    "print(set(synonyms))\n",
    "print(set(antonyms))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9090909090909091\n",
      "0.6956521739130435\n",
      "0.32\n"
     ]
    }
   ],
   "source": [
    "# 比较两个词的相似性和时态\n",
    "w1 = wordnet.synset('ship.n.01')\n",
    "w2 = wordnet.synset('boat.n.01')\n",
    "print(w1.wup_similarity(w2))\n",
    "\n",
    "\n",
    "w1 = wordnet.synset('ship.n.01')\n",
    "w2 = wordnet.synset('car.n.01')\n",
    "print(w1.wup_similarity(w2))\n",
    "\n",
    "\n",
    "w1 = wordnet.synset('ship.n.01')\n",
    "w2 = wordnet.synset('cat.n.01')\n",
    "print(w1.wup_similarity(w2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## NLTK 文本分类"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['it', 'was', 'with', 'great', 'anticipation', 'that', 'i', 'sat', 'down', 'to', 'view', 'braveheart', 'last', 'week', 'as', 'it', 'premiered', 'on', 'american', 'cable', '.', 'the', 'academy', 'award', 'winning', 'film', 'had', 'been', 'highly', 'acclaimed', '.', 'it', 'also', 'featured', 'the', 'music', 'of', 'one', 'of', 'my', 'favorite', 'film', 'composers', ',', 'james', 'horner', '.', 'what', 'i', 'was', 'in', 'for', 'was', 'a', 'disappointing', 'and', 'overlong', 'film', 'which', 'was', 'anything', 'but', 'the', 'best', 'picture', 'of', '1995', '.', 'what', 'drags', 'braveheart', 'down', 'is', 'its', 'screenplay', '.', 'it', 'abounds', 'with', 'high', 'production', 'values', ':', 'john', 'toll', \"'\", 's', 'award', '-', 'winning', 'cinematography', '(', 'which', 'also', 'graced', 'edward', 'zwick', \"'\", 's', '1994', 'legends', 'of', 'the', 'fall', ')', ',', 'a', 'gorgeous', 'score', 'by', 'horner', ',', 'and', 'the', 'sort', 'of', 'logistics', 'that', 'make', 'you', 'wish', 'assistant', 'directors', 'were', 'household', 'names', '.', 'but', 'this', 'does', 'not', 'save', 'a', 'misguided', 'script', '.', 'the', 'film', 'wishes', 'to', 'paint', 'its', 'central', 'character', 'as', 'a', 'hero', ',', 'but', 'the', 'viewer', \"'\", 's', 'only', 'response', 'to', 'his', '\"', 'heroism', '\"', 'is', 'intellectual', ':', 'william', 'wallace', '(', 'producer', '-', 'director', 'mel', 'gibson', ')', 'is', 'fighting', 'for', 'freedom', 'and', 'against', 'tyranny', ',', 'so', 'we', 'have', 'to', 'root', 'for', 'him', '.', 'but', 'wallace', \"'\", 's', 'actions', 'paint', 'a', 'different', 'story', '.', 'he', 'speaks', 'of', 'freedom', 'and', 'acts', 'of', 'vengeance', '.', 'though', 'one', 'intellectually', 'realizes', 'wallace', 'is', 'on', 'the', 'right', 'side', ',', 'the', 'film', 'paints', 'an', 'unconvincing', 'emotional', 'portait', ',', 'in', 'which', 'wallace', 'is', 'just', 'not', 'as', 'bad', 'as', 'the', 'english', 'king', '.', 'wallace', 'speaks', 'of', 'freedom', ',', 'but', 'his', 'acts', 'point', 'toward', 'vengeance', '.', 'after', 'kicking', 'the', 'english', 'out', 'of', 'scotland', ',', 'he', 'decides', 'to', 'invade', 'england', '.', 'this', 'is', 'evident', 'of', 'a', '\"', 'spartacus', 'complex', '\"', ',', 'and', 'this', 'example', 'applies', 'both', 'historically', 'and', 'cinematically', '.', 'the', 'historical', 'spartacus', 'at', 'one', 'point', 'moved', 'from', 'liberating', 'slaves', 'to', 'sacking', 'roman', 'cities', ';', 'the', 'film', 'spartacus', ',', 'like', 'braveheart', ',', 'has', 'high', 'production', 'values', 'and', 'competent', 'performaces', 'and', 'is', 'dragged', 'down', 'by', 'an', 'awful', 'screenplay', '.', 'it', 'is', 'a', 'shame', 'that', 'such', 'an', 'excellent', 'score', 'is', 'part', 'of', 'such', 'a', 'terrible', 'film', '.', 'horner', \"'\", 's', 'score', 'tries', 'to', 'make', 'emotional', 'connection', ',', 'but', 'the', 'performances', 'and', 'the', 'script', 'do', 'not', 'help', '.', 'gibson', 'portrays', 'wallace', 'in', 'such', 'a', 'way', 'that', 'the', 'audience', 'cannot', 'relate', 'to', 'him', 'or', 'identify', 'with', 'him', '.', 'this', 'drags', 'down', 'any', 'emotional', 'connection', 'to', 'the', 'film', \"'\", 's', 'plot', ',', 'and', 'turns', 'three', 'hours', 'into', 'a', 'total', 'waste', 'of', 'time', '.'], 'neg')\n",
      "[(',', 77717), ('the', 76529), ('.', 65876), ('a', 38106), ('and', 35576), ('of', 34123), ('to', 31937), (\"'\", 30585), ('is', 25195), ('in', 21822), ('s', 18513), ('\"', 17612), ('it', 16107), ('that', 15924), ('-', 15595)]\n",
      "253\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "documents = [(list(movie_reviews.words(fileld)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileld in movie_reviews.fileids(category)]\n",
    "\n",
    "random.shuffle(documents)\n",
    "\n",
    "print(documents[1])\n",
    "\n",
    "all_words = []\n",
    "for w in movie_reviews.words():\n",
    "    all_words.append(w.lower())\n",
    "\n",
    "all_words = nltk.FreqDist(all_words)\n",
    "print(all_words.most_common(15))\n",
    "print(all_words[\"stupid\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## NLTK将单词转换为特征"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "word_features = list(all_words.keys())[:3000]\n",
    "\n",
    "def find_features(document):\n",
    "    words = set(document)\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "    return features\n",
    "\n",
    "featuresets = [(find_features(rev), category) for (rev, category) in documents]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## NLTK朴素贝叶斯分类器"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier accuracy percent: 74.0\n"
     ]
    }
   ],
   "source": [
    "training_set = featuresets[:1900]\n",
    "\n",
    "testing_set = featuresets[1900:]\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "\n",
    "print(\"Classifier accuracy percent:\",(nltk.classify.accuracy(classifier, testing_set))*100)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                   sucks = True              neg : pos    =      9.9 : 1.0\n",
      "                  annual = True              pos : neg    =      9.6 : 1.0\n",
      "                 frances = True              pos : neg    =      8.9 : 1.0\n",
      "             silverstone = True              neg : pos    =      7.1 : 1.0\n",
      "              schumacher = True              neg : pos    =      7.1 : 1.0\n",
      "               atrocious = True              neg : pos    =      7.1 : 1.0\n",
      "                 idiotic = True              neg : pos    =      7.1 : 1.0\n",
      "                  shoddy = True              neg : pos    =      7.1 : 1.0\n",
      "                  kombat = True              neg : pos    =      7.1 : 1.0\n",
      "                  regard = True              pos : neg    =      6.9 : 1.0\n",
      "                  turkey = True              neg : pos    =      6.4 : 1.0\n",
      "                    mena = True              neg : pos    =      6.4 : 1.0\n",
      "               pregnancy = True              neg : pos    =      6.4 : 1.0\n",
      "                  suvari = True              neg : pos    =      6.4 : 1.0\n",
      "                obstacle = True              pos : neg    =      6.3 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(15)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## NLTK保存分类器"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "save_classifier = open(\"naivebayes.pickle\",\"wb\")\n",
    "pickle.dump(classifier, save_classifier)\n",
    "save_classifier.close()\n",
    "\n",
    "classifier_f = open(\"naivebayes.pickle\", \"rb\")\n",
    "classifier = pickle.load(classifier_f)\n",
    "classifier_f.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## NLTK 和 Sklearn\n",
    "可以在NLTK中使用sklearn模块的分类器"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB accuracy percent: 0.75\n",
      "BernoulliNB accuracy percent: 0.74\n",
      "LogisticRegression_classifier accuracy percent: 80.0\n",
      "SGDClassifier_classifier accuracy percent: 76.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "\n",
    "MNB_classifier = SklearnClassifier(MultinomialNB())\n",
    "MNB_classifier.train(training_set)\n",
    "print(\"MultinomialNB accuracy percent:\",nltk.classify.accuracy(MNB_classifier, testing_set))\n",
    "\n",
    "BNB_classifier = SklearnClassifier(BernoulliNB())\n",
    "BNB_classifier.train(training_set)\n",
    "print(\"BernoulliNB accuracy percent:\",nltk.classify.accuracy(BNB_classifier, testing_set))\n",
    "\n",
    "LogisticRegression_classifier = SklearnClassifier(LogisticRegression())\n",
    "LogisticRegression_classifier.train(training_set)\n",
    "print(\"LogisticRegression_classifier accuracy percent:\", (nltk.classify.accuracy(LogisticRegression_classifier, testing_set))*100)\n",
    "\n",
    "SGDClassifier_classifier = SklearnClassifier(SGDClassifier())\n",
    "SGDClassifier_classifier.train(training_set)\n",
    "print(\"SGDClassifier_classifier accuracy percent:\", (nltk.classify.accuracy(SGDClassifier_classifier, testing_set))*100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 使用NLTK来组合算法"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC_classifier accuracy percent: 78.0\n",
      "NuSVC_classifier accuracy percent: 80.0\n",
      "voted_classifier accuracy percent: 78.0\n",
      "Classification: pos Confidence %: 57.14285714285714\n",
      "Classification: neg Confidence %: 100.0\n",
      "Classification: pos Confidence %: 85.71428571428571\n",
      "Classification: neg Confidence %: 100.0\n",
      "Classification: neg Confidence %: 100.0\n",
      "Classification: neg Confidence %: 100.0\n"
     ]
    }
   ],
   "source": [
    "from nltk.classify import ClassifierI\n",
    "from statistics import mode\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "\n",
    "class VoteClassifier(ClassifierI):\n",
    "    def labels(self):\n",
    "        pass\n",
    "\n",
    "    def __init__(self, *classifiers):\n",
    "        self._classifiers = classifiers\n",
    "\n",
    "    def classify(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "        return mode(votes)\n",
    "\n",
    "    def confidence(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "\n",
    "        choice_votes = votes.count(mode(votes))\n",
    "        conf = choice_votes / len(votes)\n",
    "        return conf\n",
    "\n",
    "##SVC_classifier = SklearnClassifier(SVC())\n",
    "##SVC_classifier.train(training_set)\n",
    "##print(\"SVC_classifier accuracy percent:\", (nltk.classify.accuracy(SVC_classifier, testing_set))*100)\n",
    "\n",
    "LinearSVC_classifier = SklearnClassifier(LinearSVC())\n",
    "LinearSVC_classifier.train(training_set)\n",
    "print(\"LinearSVC_classifier accuracy percent:\", (nltk.classify.accuracy(LinearSVC_classifier, testing_set))*100)\n",
    "\n",
    "NuSVC_classifier = SklearnClassifier(NuSVC())\n",
    "NuSVC_classifier.train(training_set)\n",
    "print(\"NuSVC_classifier accuracy percent:\", (nltk.classify.accuracy(NuSVC_classifier, testing_set))*100)\n",
    "\n",
    "voted_classifier = VoteClassifier(classifier,\n",
    "                                  NuSVC_classifier,\n",
    "                                  LinearSVC_classifier,\n",
    "                                  SGDClassifier_classifier,\n",
    "                                  MNB_classifier,\n",
    "                                  BNB_classifier,\n",
    "                                  LogisticRegression_classifier)\n",
    "\n",
    "print(\"voted_classifier accuracy percent:\", (nltk.classify.accuracy(voted_classifier, testing_set))*100)\n",
    "\n",
    "print(\"Classification:\", voted_classifier.classify(testing_set[0][0]), \"Confidence %:\",voted_classifier.confidence(testing_set[0][0])*100)\n",
    "print(\"Classification:\", voted_classifier.classify(testing_set[1][0]), \"Confidence %:\",voted_classifier.confidence(testing_set[1][0])*100)\n",
    "print(\"Classification:\", voted_classifier.classify(testing_set[2][0]), \"Confidence %:\",voted_classifier.confidence(testing_set[2][0])*100)\n",
    "print(\"Classification:\", voted_classifier.classify(testing_set[3][0]), \"Confidence %:\",voted_classifier.confidence(testing_set[3][0])*100)\n",
    "print(\"Classification:\", voted_classifier.classify(testing_set[4][0]), \"Confidence %:\",voted_classifier.confidence(testing_set[4][0])*100)\n",
    "print(\"Classification:\", voted_classifier.classify(testing_set[5][0]), \"Confidence %:\",voted_classifier.confidence(testing_set[5][0])*100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}