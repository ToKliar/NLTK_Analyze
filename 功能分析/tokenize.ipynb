{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK基于不同的算法实现tokenizer，包括分句器、多种分词器、还有一些特殊的tokenizer，并在基础分词器、分局器的基础上进行扩展  \n",
    "本部分主要介绍基础分词器的用法，并比较其效果  \n",
    "NLTK中的基础分词器包括：\n",
    "+ RegexpTokenizer 使用正则表达式分词\n",
    "+ ReppTokenizer Repp分词器 http://anthology.aclweb.org/P/P12/P12-2.pdf#page=406\n",
    "+ ToktokTokenizer 通用分词器\n",
    "+ TreebankWordTokenizer 使用正则表达式对文本进行标记之后分词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK使用PunktSentenceTokenizer进行分句     \n",
    "这是一种使用无监督算法通过缩写词、固定搭配和某些开始句子的固定的词进行训练的分句器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"PRESIDENT GEORGE W. BUSH'S ADDRESS BEFORE A JOINT SESSION OF THE CONGRESS ON THE STATE OF THE UNION\\n \\nJanuary 31, 2006\\n\\nTHE PRESIDENT: Thank you all.\", 'Mr. Speaker, Vice President Cheney, members of Congress, members of the Supreme Court and diplomatic corps, distinguished guests, and fellow citizens: Today our nation lost a beloved, graceful, courageous woman who called America to its founding ideals and carried on a noble dream.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
    "\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "print(tokenized[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK支持简单分词（按照空格分词）以及按照行/空格/制表符进行分词的简单分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Is', '9.5', 'or', '525,600', 'my', 'favorite', 'number', '?', 'May', 'I', 'help', 'you', '?']\n",
      "['Is 9.5 or 525,600 my favorite number?', 'May I help you?']\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Is 9.5 or 525,600 my favorite number?\n",
    "May I help you?\n",
    "\"\"\"\n",
    "print(nltk.tokenize.casual_tokenize(text))\n",
    "print(nltk.tokenize.LineTokenizer().tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Is', '9', '.5', 'or', '525', ',600', 'my', 'favorite', 'number', '?', 'May', 'I', 'help', 'you', '?']\n"
     ]
    }
   ],
   "source": [
    "regexp_tokenizer = nltk.tokenize.RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\n",
    "print(regexp_tokenizer.tokenize(text)) # 简单的根据正则表达式分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-8795f8374a44>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;34m'We evaluated our method on three languages and obtained error rates of 0.27% (English), 0.35% (Dutch) and 0.76% (Italian) for our best models.'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m ]\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mReppTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# doctest: +SKIP\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msents\u001b[0m\u001b[1;33m:\u001b[0m                             \u001b[1;31m# doctest: +SKIP\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m                   \u001b[1;31m# doctest: +SKIP\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\nltk\\tokenize\\repp.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, repp_dir, encoding)\u001b[0m\n\u001b[0;32m     57\u001b[0m     \"\"\"\n\u001b[0;32m     58\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrepp_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepp_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_repptokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrepp_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m         \u001b[1;31m# Set a directory to store the temporary files.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mworking_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtempfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgettempdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\nltk\\tokenize\\repp.py\u001b[0m in \u001b[0;36mfind_repptokenizer\u001b[1;34m(self, repp_dirname)\u001b[0m\n\u001b[0;32m    147\u001b[0m             \u001b[0m_repp_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_dir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrepp_dirname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv_vars\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'REPP_TOKENIZER'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m         \u001b[1;31m# Checks for the REPP binary and erg/repp.set config file.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m         \u001b[1;32massert\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_repp_dir\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'/src/repp'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    150\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_repp_dir\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'/erg/repp.set'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_repp_dir\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sents = ['Tokenization is widely regarded as a solved problem due to the high accuracy that rulebased tokenizers achieve.' ,\n",
    "'But rule-based tokenizers are hard to maintain and their rules language specific.' ,\n",
    "'We evaluated our method on three languages and obtained error rates of 0.27% (English), 0.35% (Dutch) and 0.76% (Italian) for our best models.'\n",
    "]\n",
    "tokenizer = nltk.tokenize.ReppTokenizer('./') # doctest: +SKIP\n",
    "for sent in sents:                             # doctest: +SKIP\n",
    "    tokenizer.tokenize(sent)                   # doctest: +SKIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Is', '9.5', 'or', '525,600', 'my', 'favorite', 'number', '?']\n"
     ]
    }
   ],
   "source": [
    "toktok = nltk.tokenize.ToktokTokenizer() # TikTok通用分词器\n",
    "text = u'Is 9.5 or 525,600 my favorite number?'\n",
    "print(toktok.tokenize(text, return_str=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York.', 'Please', 'buy', 'me', 'two', 'of', 'them.', 'Thanks', '.']\n"
     ]
    }
   ],
   "source": [
    "s = '''Good muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them.\\nThanks.'''\n",
    "print(nltk.tokenize.TreebankWordTokenizer().tokenize(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK中还包括一些功能特殊的分词器 \n",
    "+ SExprTokenizer 将句子按照括号分割\n",
    "+ SyllableTokenizer 将词按照音节划分\n",
    "+ TextTilingTokenizer 将文本中的每一段划分到其对应的子主题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['(a (b c))', 'd', 'e', '(f)']\n"
     ]
    }
   ],
   "source": [
    "text = '(a (b c)) d e (f)'\n",
    "print(nltk.tokenize.SExprTokenizer().tokenize(text)) # 按照括号分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'SyllableTokenizer' from 'nltk.tokenize' (d:\\Anaconda\\lib\\site-packages\\nltk\\tokenize\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)\n",
      "\u001b[1;32m<ipython-input-10-f6978c83e147>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSyllableTokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mSSP\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSyllableTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m      3\u001b[0m \u001b[0mSSP\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"justification\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'SyllableTokenizer' from 'nltk.tokenize' (d:\\Anaconda\\lib\\site-packages\\nltk\\tokenize\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import SyllableTokenizer\n",
    "SSP = SyllableTokenizer()\n",
    "SSP.tokenize(\"justification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "tt = nltk.tokenize.TextTilingTokenizer(demo_mode=True) # 将文档标记到对应的子主题\n",
    "text = brown.raw()[:4000]\n",
    "s, ss, d, b = tt.tokenize(text)\n",
    "b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit ('Anaconda')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88279d2366fe020547cde40dd65aa0e3aa662a6ec1f3ca12d88834876c85e1a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
