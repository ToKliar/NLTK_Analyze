{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK基于不同的算法实现tokenizer，包括分句器、多种分词器、还有一些特殊的tokenizer，并在基础分词器、分局器的基础上进行扩展  \n",
    "本部分主要介绍基础分词器的用法，并比较其效果  \n",
    "NLTK中的基础分词器包括：\n",
    "+ RegexpTokenizer 使用正则表达式分词\n",
    "+ ReppTokenizer Repp分词器 http://anthology.aclweb.org/P/P12/P12-2.pdf#page=406\n",
    "+ ToktokTokenizer 通用分词器\n",
    "+ TreebankWordTokenizer 使用正则表达式对文本进行标记之后分词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK使用PunktSentenceTokenizer进行分句     \n",
    "这是一种使用无监督算法通过缩写词、固定搭配和某些开始句子的固定的词进行训练的分句器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"PRESIDENT GEORGE W. BUSH'S ADDRESS BEFORE A JOINT SESSION OF THE CONGRESS ON THE STATE OF THE UNION\\n \\nJanuary 31, 2006\\n\\nTHE PRESIDENT: Thank you all.\", 'Mr. Speaker, Vice President Cheney, members of Congress, members of the Supreme Court and diplomatic corps, distinguished guests, and fellow citizens: Today our nation lost a beloved, graceful, courageous woman who called America to its founding ideals and carried on a noble dream.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
    "\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "print(tokenized[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK支持简单分词（按照空格分词）以及按照行/空格/制表符进行分词的简单分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Is', '9.5', 'or', '525,600', 'my', 'favorite', 'number', '?', 'May', 'I', 'help', 'you', '?']\n",
      "['Is 9.5 or 525,600 my favorite number?', 'May I help you?']\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Is 9.5 or 525,600 my favorite number?\n",
    "May I help you?\n",
    "\"\"\"\n",
    "print(nltk.tokenize.casual_tokenize(text))\n",
    "print(nltk.tokenize.LineTokenizer().tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Is', '9', '.5', 'or', '525', ',600', 'my', 'favorite', 'number', '?', 'May', 'I', 'help', 'you', '?']\n"
     ]
    }
   ],
   "source": [
    "regexp_tokenizer = nltk.tokenize.RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\n",
    "print(regexp_tokenizer.tokenize(text)) # 简单的根据正则表达式分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Is', '9.5', 'or', '525,600', 'my', 'favorite', 'number', '?']\n"
     ]
    }
   ],
   "source": [
    "toktok = nltk.tokenize.ToktokTokenizer() # TikTok通用分词器\n",
    "text = u'Is 9.5 or 525,600 my favorite number?'\n",
    "print(toktok.tokenize(text, return_str=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York.', 'Please', 'buy', 'me', 'two', 'of', 'them.', 'Thanks', '.']\n"
     ]
    }
   ],
   "source": [
    "s = '''Good muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them.\\nThanks.'''\n",
    "print(nltk.tokenize.TreebankWordTokenizer().tokenize(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK中还包括一些功能特殊的分词器 \n",
    "+ SExprTokenizer 将句子按照括号分割\n",
    "+ SyllableTokenizer 将词按照音节划分\n",
    "+ TextTilingTokenizer 将文本中的每一段划分到其对应的子主题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['(a (b c))', 'd', 'e', '(f)']\n"
     ]
    }
   ],
   "source": [
    "text = '(a (b c)) d e (f)'\n",
    "print(nltk.tokenize.SExprTokenizer().tokenize(text)) # 按照括号分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jus', 'ti', 'fi', 'ca', 'tion']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import SyllableTokenizer\n",
    "SSP = SyllableTokenizer()\n",
    "SSP.tokenize(\"justification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "import numpy\n",
    "tt = nltk.tokenize.TextTilingTokenizer(demo_mode=True) # 将文档标记到对应的子主题\n",
    "text = brown.raw()[:4000]\n",
    "s, ss, d, b = tt.tokenize(text)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "88279d2366fe020547cde40dd65aa0e3aa662a6ec1f3ca12d88834876c85e1a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
